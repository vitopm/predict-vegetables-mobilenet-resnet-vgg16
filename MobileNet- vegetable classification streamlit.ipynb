{"cells":[{"cell_type":"code","execution_count":null,"id":"x9OEnj7gLgy_","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18373,"status":"ok","timestamp":1673084925490,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"x9OEnj7gLgy_","outputId":"61f98cf6-6b0a-4a9d-e899-fd345ecb2616"},"outputs":[],"source":["! pip install kaggle\n","\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json # upload key kaggle ke colab\n","\n","!kaggle datasets download -d misrakahmed/vegetable-image-dataset # download kaggle dataset\n","!unzip vegetable-image-dataset.zip # unzip dataset"]},{"cell_type":"code","execution_count":null,"id":"c9f0ee87","metadata":{"executionInfo":{"elapsed":585,"status":"ok","timestamp":1673084926070,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"c9f0ee87","scrolled":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"0ea89589","metadata":{"id":"0ea89589"},"source":["# Problem Statement\n","\n","<li>VegCart(Random Name) is a fresh produce supply chain company. They are pioneers in solving one of the toughest supply chain problems of the world by leveraging innovative technology. They source fresh produce from farmers and deliver them to businesses within 12 hours. An integral component of their automation process is the development of robust classifiers which can distinguish between images of different types of vegetables, while also correctly labeling images that do not contain any one type of vegetable as noise.</li>\n","\n","<li>As a starting point, we have been tasked with preparing a multiclass classifier for identifying these vegetables. The dataset provided has all the required images to achieve the task.</li>"]},{"cell_type":"code","execution_count":23,"id":"b155607a","metadata":{"executionInfo":{"elapsed":3479,"status":"ok","timestamp":1673084929546,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"b155607a"},"outputs":[],"source":["import os\n","import glob\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import sklearn.metrics as metrics\n","\n","\n","# Tensorflow import\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, ReLU, Softmax, BatchNormalization, Dropout\n","from tensorflow.random import set_seed\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array"]},{"cell_type":"code","execution_count":null,"id":"a262118f","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1673084929547,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"a262118f"},"outputs":[],"source":["def training_plot(metrics, history):\n","  f, ax = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 5))\n","  for idx, metric in enumerate(metrics):\n","    ax[idx].plot(history.history[metric], ls='dashed')\n","    ax[idx].set_xlabel(\"Epochs\")\n","    ax[idx].set_ylabel(metric)\n","    ax[idx].plot(history.history['val_' + metric]);\n","    ax[idx].legend([metric, 'val_' + metric])\n","    \n","def ConfusionMatrix(model, ds, label_list):\n","# Note: This logic doesn't work with shuffled datasets\n","    plt.figure(figsize=(15,15))\n","    y_pred = model.predict(ds)\n","    predicted_categories = tf.argmax(y_pred, axis=1)\n","    true_categories = tf.concat([y for x, y in ds], axis=0)\n","    cm = metrics.confusion_matrix(true_categories,predicted_categories) # last batch \n","    sns.heatmap(cm, annot=True, xticklabels=label_list, yticklabels=label_list, cmap=\"YlGnBu\", fmt='g')\n","    plt.show()\n","    \n","def testAccuracy(model):\n","    true_categories = tf.concat([y for x, y in test_ds], axis=0)\n","    images = tf.concat([x for x, y in test_ds], axis=0)\n","    y_pred = model.predict(test_ds)\n","    class_names = test_data.class_names\n","    predicted_categories = tf.argmax(y_pred, axis=1)\n","    test_acc = metrics.accuracy_score(true_categories, predicted_categories) * 100\n","    print(f'\\nTest Accuracy: {test_acc:.2f}%\\n')\n","    \n","def plot_image(pred_array, true_label, img, class_names):\n","  plt.grid(False)\n","  plt.xticks([])\n","  plt.yticks([])\n","\n","  plt.imshow(img, cmap=plt.cm.binary)\n","\n","  predicted_label = np.argmax(pred_array)\n","  if predicted_label == true_label:\n","    color = 'blue'\n","  else:\n","    color = 'red'\n","\n","  plt.xlabel(\"{} {:2.0f}% \".format(class_names[predicted_label],\n","                                100*np.max(pred_array),\n","                                ),\n","                                color=color)\n","\n","def predictions(model):\n","    true_categories = tf.concat([y for x, y in test_ds], axis=0)\n","    images = tf.concat([x for x, y in test_ds], axis=0)\n","    y_pred = model.predict(test_ds)\n","    class_names = test_data.class_names\n","    # Randomly sample 15 test images and plot it with their predicted labels, and the true labels.\n","    indices = random.sample(range(len(images)), 15)\n","    # Color correct predictions in blue and incorrect predictions in red.\n","    num_rows = 5\n","    num_cols = 3\n","    num_images = num_rows*num_cols\n","    plt.figure(figsize=(4*num_cols, 2*num_rows))\n","    for i,index in enumerate(indices):\n","      plt.subplot(num_rows, num_cols, i+1)\n","      plot_image(y_pred[index], true_categories[index], images[index],class_names)\n","    plt.tight_layout()\n","    plt.show()\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\"final_model.h5\", save_best_only=True)\n","\n","early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\",patience=5, restore_best_weights=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"c01bc31c","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1673084929547,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"c01bc31c"},"outputs":[],"source":["set_seed(111) # set random seed\n","\n","# To supress any warnings during the flow\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"id":"6eaf60bd","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1673084929548,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"6eaf60bd"},"outputs":[],"source":["class_dirs = os.listdir(\"Vegetable Images/train\") # list all directories inside \"train\" folder\n","\n","image_dict = {} # dict to store image array(key) for every class(value)\n","\n","count_dict = {} # dict to store count of files(key) for every class(value)\n","\n","# iterate over all class_dirs\n","for cls in class_dirs:\n","    # get list of all paths inside the subdirectory\n","    file_paths = glob.glob(f'Vegetable Images/train/{cls}/*')\n","    # count number of files in each class and add it to count_dict\n","    count_dict[cls] = len(file_paths)\n","    # select random item from list of image paths\n","    image_path = random.choice(file_paths)\n","    # load image using keras utility function and save it in image_dict\n","    image_dict[cls] = tf.keras.utils.load_img(image_path)"]},{"cell_type":"markdown","id":"c505e6d8","metadata":{"id":"c505e6d8"},"source":["# Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"id":"f864cef9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":699},"executionInfo":{"elapsed":2117,"status":"ok","timestamp":1673084931657,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"f864cef9","outputId":"d96050d6-cdef-4a5c-b3a1-e835145c17eb"},"outputs":[],"source":["## Viz Random Sample from each class\n","plt.figure(figsize=(15, 12))\n","# iterate over dictionary items (class label, image array)\n","for i, (cls,img) in enumerate(image_dict.items()): \n","    # create a subplot axis\n","    ax = plt.subplot(4, 4, i + 1)\n","    # plot each image\n","    plt.imshow(img)\n","    # set \"class name\" along with \"image size\" as title \n","    plt.title(f'{cls}, {img.size}')\n","    plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"id":"42fe77c5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1673084931658,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"42fe77c5","outputId":"59316c21-030d-4d3e-f8e2-020884cd8099"},"outputs":[],"source":["## Let's now Plot the Data Distribution of Training Data across Classes\n","df_count_train = pd.DataFrame({\n","    \"class\": count_dict.keys(),     # keys of count_dict are class labels\n","    \"count\": count_dict.values(),   # value of count_dict contain counts of each class\n","})\n","print(\"Count of training samples per class:\\n\", df_count_train)"]},{"cell_type":"code","execution_count":null,"id":"698af25b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1673084931659,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"698af25b","outputId":"2fdfb282-74ae-4af2-d81c-bb4a1be7f73f"},"outputs":[],"source":["# draw a bar plot using pandas in-built plotting function\n","plt.figure(figsize=(15,12))\n","df_count_train.plot.bar(x='class', y='count', title=\"Training Data Count per class\") \n","plt.show()"]},{"cell_type":"markdown","id":"5c39e663","metadata":{"id":"5c39e663"},"source":["# Loading data set"]},{"cell_type":"code","execution_count":null,"id":"06421a5f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3418,"status":"ok","timestamp":1673084935064,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"06421a5f","outputId":"b2604728-ac26-4850-e9c8-02cfc0a066e3"},"outputs":[],"source":["print('\\nLoading Train Data...')\n","train_data = tf.keras.utils.image_dataset_from_directory(\n","    \"Vegetable Images/train\", shuffle = True,\n",")\n","\n","print('\\nLoading Validation Data...')\n","val_data = tf.keras.utils.image_dataset_from_directory(\n","    \"Vegetable Images/validation\", shuffle = False,\n",")\n","\n","print('\\nLoading Test Data...')\n","test_data = tf.keras.utils.image_dataset_from_directory(\n","    \"Vegetable Images/test\", shuffle = False,\n",")"]},{"cell_type":"markdown","id":"d9b98ae0","metadata":{"id":"d9b98ae0"},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":26,"id":"d15cd960","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1673084935442,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"d15cd960"},"outputs":[{"ename":"NameError","evalue":"name 'train_data' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[26], line 13\u001b[0m\n\u001b[0;32m      4\u001b[0m data_preprocess \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m      5\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata_preprocess\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     layers\u001b[39m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     ]\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[39m# Perform Data Processing on the train, val, test dataset\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m train_ds \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (data_preprocess(x), y))\n\u001b[0;32m     14\u001b[0m val_ds \u001b[39m=\u001b[39m val_data\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (data_preprocess(x), y))\n\u001b[0;32m     15\u001b[0m test_ds \u001b[39m=\u001b[39m test_data\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (data_preprocess(x), y))\n","\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"]}],"source":["height, width = 224, 224\n","\n","# Data Processing Stage with resizing and rescaling operations\n","data_preprocess = keras.Sequential(\n","    name=\"data_preprocess\",\n","    layers=[\n","        layers.Resizing(height, width), # Shape Preprocessing\n","        layers.Rescaling(1.0/255), # Value Preprocessing\n","    ]\n",")\n","\n","# Perform Data Processing on the train, val, test dataset\n","train_ds = train_data.map(lambda x, y: (data_preprocess(x), y))\n","val_ds = val_data.map(lambda x, y: (data_preprocess(x), y))\n","test_ds = test_data.map(lambda x, y: (data_preprocess(x), y))"]},{"cell_type":"code","execution_count":24,"id":"795050f8","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1673084935442,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"795050f8"},"outputs":[],"source":["num_classes = 15\n","hidden_size_1 = 1024\n","hidden_size_2 = 256"]},{"cell_type":"markdown","id":"121774a0","metadata":{"id":"121774a0"},"source":["# MobileNetV2"]},{"cell_type":"code","execution_count":32,"id":"e3f87aae","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2315,"status":"ok","timestamp":1673084937753,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"e3f87aae","outputId":"a0c4d257-5e34-48ac-ee2e-ae6783684b60"},"outputs":[{"ename":"ValueError","evalue":"When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (224, 224, 3).  Received: input_shape=[224, 224, 3]","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m height, width \u001b[39m=\u001b[39m \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m\n\u001b[1;32m----> 3\u001b[0m pretrained_mobilenet_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mapplications\u001b[39m.\u001b[39;49mMobileNetV2(weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m, include_top\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, input_shape\u001b[39m=\u001b[39;49m[height,width, \u001b[39m3\u001b[39;49m])\n\u001b[0;32m      4\u001b[0m pretrained_mobilenet_model\u001b[39m.\u001b[39mtrainable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      5\u001b[0m mobilenet_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m      6\u001b[0m     pretrained_mobilenet_model,\n\u001b[0;32m      7\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mGlobalAveragePooling2D(),\n\u001b[0;32m      8\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m15\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m ])\n","File \u001b[1;32mc:\\Users\\vito pm\\anaconda3\\envs\\test_dl_st_2\\lib\\site-packages\\keras\\applications\\mobilenet_v2.py:300\u001b[0m, in \u001b[0;36mMobileNetV2\u001b[1;34m(input_shape, alpha, include_top, weights, input_tensor, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m         default_size \u001b[39m=\u001b[39m \u001b[39m224\u001b[39m\n\u001b[1;32m--> 300\u001b[0m input_shape \u001b[39m=\u001b[39m imagenet_utils\u001b[39m.\u001b[39;49mobtain_input_shape(\n\u001b[0;32m    301\u001b[0m     input_shape,\n\u001b[0;32m    302\u001b[0m     default_size\u001b[39m=\u001b[39;49mdefault_size,\n\u001b[0;32m    303\u001b[0m     min_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m    304\u001b[0m     data_format\u001b[39m=\u001b[39;49mbackend\u001b[39m.\u001b[39;49mimage_data_format(),\n\u001b[0;32m    305\u001b[0m     require_flatten\u001b[39m=\u001b[39;49minclude_top,\n\u001b[0;32m    306\u001b[0m     weights\u001b[39m=\u001b[39;49mweights,\n\u001b[0;32m    307\u001b[0m )\n\u001b[0;32m    309\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39mimage_data_format() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mchannels_last\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    310\u001b[0m     row_axis, col_axis \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Users\\vito pm\\anaconda3\\envs\\test_dl_st_2\\lib\\site-packages\\keras\\applications\\imagenet_utils.py:367\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[1;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[39mif\u001b[39;00m input_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m         \u001b[39mif\u001b[39;00m input_shape \u001b[39m!=\u001b[39m default_shape:\n\u001b[1;32m--> 367\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    368\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mWhen setting `include_top=True` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    369\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mand loading `imagenet` weights, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`input_shape` should be \u001b[39m\u001b[39m{\u001b[39;00mdefault_shape\u001b[39m}\u001b[39;00m\u001b[39m.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived: input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m             )\n\u001b[0;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m default_shape\n\u001b[0;32m    374\u001b[0m \u001b[39mif\u001b[39;00m input_shape:\n","\u001b[1;31mValueError\u001b[0m: When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (224, 224, 3).  Received: input_shape=[224, 224, 3]"]}],"source":["height, width = 224, 224\n","\n","pretrained_mobilenet_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=[height,width, 3])\n","pretrained_mobilenet_model.trainable=False\n","mobilenet_model = tf.keras.Sequential([\n","    pretrained_mobilenet_model,\n","    tf.keras.layers.GlobalAveragePooling2D(),\n","    tf.keras.layers.Dense(15, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":28,"id":"70b1afc2","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1673084937754,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"70b1afc2"},"outputs":[],"source":["mobilenet_model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":29,"id":"d3f26c64","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1673084937754,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"d3f26c64","outputId":"e82b3e45-980f-428e-dc48-3103849e4049"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n"," ional)                                                          \n","                                                                 \n"," global_average_pooling2d (G  (None, 1280)             0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dense (Dense)               (None, 15)                19215     \n","                                                                 \n","=================================================================\n","Total params: 2,277,199\n","Trainable params: 19,215\n","Non-trainable params: 2,257,984\n","_________________________________________________________________\n"]}],"source":["mobilenet_model.summary()"]},{"cell_type":"code","execution_count":31,"id":"qvCJq3BiFy0W","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"elapsed":864,"status":"ok","timestamp":1673084938611,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"qvCJq3BiFy0W","outputId":"02cde1df-0911-4a3b-b4f1-fbc9645c564a"},"outputs":[{"name":"stdout","output_type":"stream","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"]}],"source":["tf.keras.utils.plot_model(mobilenet_model, to_file=\"mobilenet_model.png\", show_shapes=True)"]},{"cell_type":"code","execution_count":null,"id":"7de6caf6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":433051,"status":"ok","timestamp":1673085371655,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"7de6caf6","outputId":"be887a5a-00c9-4866-b2a1-9e5670b00256"},"outputs":[],"source":["history_mobilenet = mobilenet_model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[checkpoint_callback,early_stopping_callback])"]},{"cell_type":"code","execution_count":null,"id":"59872d5b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"elapsed":817,"status":"ok","timestamp":1673085372465,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"59872d5b","outputId":"41f04b24-221e-416e-ff12-ee630ccb360e"},"outputs":[],"source":["training_plot(['loss', 'accuracy'], history_mobilenet)"]},{"cell_type":"code","execution_count":null,"id":"85bcc060","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16347,"status":"ok","timestamp":1673085388805,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"85bcc060","outputId":"4e6d2bbe-28dd-44cc-afeb-662a87067dc8"},"outputs":[],"source":["testAccuracy(mobilenet_model)"]},{"cell_type":"code","execution_count":null,"id":"bc48d29c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":939},"executionInfo":{"elapsed":15564,"status":"ok","timestamp":1673085404357,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"bc48d29c","outputId":"74eef162-1917-4bd2-ed26-589820b82c70"},"outputs":[],"source":["ConfusionMatrix(mobilenet_model, test_ds, test_data.class_names)"]},{"cell_type":"markdown","id":"PIgYMvEWbi-k","metadata":{"id":"PIgYMvEWbi-k"},"source":["# Saving model (Download)"]},{"cell_type":"code","execution_count":null,"id":"GXMcsTzE3eyc","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1673085404358,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"GXMcsTzE3eyc"},"outputs":[],"source":["mobilenet_model.save('mobilenet_model_v2.h5') #tolong download file ini"]},{"attachments":{},"cell_type":"markdown","id":"daf7508c","metadata":{},"source":["# Load model"]},{"cell_type":"code","execution_count":5,"id":"ej1pQxsr3l-A","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1673085404359,"user":{"displayName":"Vito PM","userId":"03226313192724136535"},"user_tz":-420},"id":"ej1pQxsr3l-A"},"outputs":[],"source":["model = tf.keras.models.load_model('./models/mobilenet_model_v2.h5')"]},{"cell_type":"code","execution_count":null,"id":"8959ed4f","metadata":{},"outputs":[],"source":["def predict_image(filename,model):\n","    img_ = image.load_img(filename, target_size=(224, 224))\n","    img_array = image.img_to_array(img_)\n","    img_processed = np.expand_dims(img_array, axis=0) \n","    img_processed /= 255.\n","\n","    prediction = model.predict(img_processed)\n","    index = np.argmax(prediction)\n","\n","    # show result\n","    col11, col12 = st.columns(2)\n","    with col11:\n","        st.write('#### Your image')\n","        st.image(filename, caption=filename.name)\n","\n","    with col12:\n","        st.write(f'#### Prediction: {category[index]}')\n","        fig, ax = plt.subplots()\n","        plt.title(\"Prediction - {}\".format(category[index]))\n","        plt.axis('off')\n","        plt.imshow(img_array)\n","        st.pyplot(fig)\n","    st.write('----')\n","    st.write('🍅🍆🥒🥕🥔🥜🍅🍆🥒🥕🥔🥜🍅🍆🥒🥕🥔🥜🍅🍆🥒🥕🥔🥜🍅🍆🥒🥕🥔🥜🍅🍆')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"test_dl_st_2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"papermill":{"default_parameters":{},"duration":11045.883556,"end_time":"2022-10-28T20:58:30.863665","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-10-28T17:54:24.980109","version":"2.3.4"},"vscode":{"interpreter":{"hash":"258649672b533d46569488bf8c4294479ad02725e769301cb6e412c61c96ad9e"}}},"nbformat":4,"nbformat_minor":5}
